**架构——哨兵**

# 1、简介

- Redis的主从复制模式下，一旦主节点由于故障不能提供服务，需要人 工将从节点晋升为主节点，

同时还要通知应用方更新主节点地址，对于很多 应用场景这种故障处理的方式是无法接受的。

- Redis较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。

# 2、工作原理

- 每个哨兵(sentinel) 会向其它哨兵(sentinel)、master、slave定时发送消息,以确认对方是否”活”着,

如果发现对方在 指定时间( down-after-milliseconds项 )内未回应,则暂时认为对方已挂(主观宕

机:sdown)，若“哨兵群”中的多数 sentinel,都报告某一master没响应,系统才认为该master”彻底死

亡”(即:客观上的真正down机:odown),通过一定的 vote算法,从剩下的slave节点中,选一台提升为

master,然后自动修改相关配置。

![](images/WEBRESOURCE0467a464161d640acc7604ce26cb9f62截图.png)

# 3、实验部署

## 1）部署哨兵1：监听26378端口

```
#!/bin/bash
cat << EOF > /usr/local/redis/conf/redis-sentinel26738.conf
port 26378
daemonize yes
dir "/tmp"
sentinel monitor mymaster 127.0.0.1 6380 2
sentinel down-after-milliseconds mymaster 60000
sentinel failover-timeout mymaster 180000
sentinel parallel-syncs mymaster 1
logfile "/usr/local/redis/log/sentinel.log.26378"
EOF
```

## 2）配置哨兵2：监听26379端口

```
#!/bin/bash
cat << EOF > /usr/local/redis/conf/redis-sentinel26739.conf
port 26379
daemonize yes
dir "/tmp"
sentinel monitor mymaster 127.0.0.1 6380 2
sentinel down-after-milliseconds mymaster 60000
sentinel failover-timeout mymaster 180000
sentinel parallel-syncs mymaster 1
logfile "/usr/local/redis/log/sentinel.log.26379"
EOF
```

## 3）启动哨兵服务

```
[root@master redis]# redis-sentinel /usr/local/redis/conf/redis-
sentinel26739.conf
[root@master redis]# redis-sentinel /usr/local/redis/conf/redis-
sentinel26738.conf
```

- 通常情况下我们尽量启动3个哨兵(奇数)及以上监听一个主节点。

- 验证测试

```
关闭master上redis服务
查看日志字段：
+switch-master mymaster 127.0.0.1 6380 127.0.0.1 6382
master切换
重启旧master上redis服务
查看日志字段：
+convert-to-slave slave 127.0.0.1:6380 127.0.0.1 6380 @ mymaster 127.0.0.1
6382
成为slave
127.0.0.1:6382> info Replication
# Replication
role:master
connected_slaves:2
slave0:ip=127.0.0.1,port=6381,state=online,offset=131255,lag=0
slave1:ip=127.0.0.1,port=6380,state=online,offset=131388,lag=0
master_repl_offset:131388
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:2
repl_backlog_histlen:131387
```