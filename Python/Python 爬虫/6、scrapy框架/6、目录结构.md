# post请求实例

```
import scrapy


import json

class TestpostSpider(scrapy.Spider):
    name = 'testpost'
    allowed_domains = ['https://fanyi.baidu.com/sug']
    # post请求 如果没有参数 那么这个请求将没有任何意义
    # 所以start_urls 也没有用了
    # parse方法也没有用了
    # start_urls = ['https://fanyi.baidu.com/sug/']
    #
    # def parse(self, response):
    #     pass

    def start_requests(self):
        url = 'https://fanyi.baidu.com/sug'

        data = {
            'kw': 'final'
        }

        yield scrapy.FormRequest(url=url,formdata=data,callback=self.parse_second)

    def parse_second(self,response):

        content = response.text
        obj = json.loads(content,encoding='utf-8')

        print(obj)

```

# 应用实例

## spiders.movie.py

```
import scrapy
from project_name.items import ProjectNameItem
class MovieSpider(scrapy.Spider):
    name = 'movie'
    allowed_domains = ['www.dytt8.net']
    start_urls = ['https://www.dytt8.net/html/gndy/china/index.html']
    def parse(self, response):
        a_list=response.xpath("//table//tr[2]//a[2]")
        for a in a_list:
            name=a.xpath("./text()").extract()[0]
            # print(name)
            src="https://www.dytt8.net"+a.xpath("./@href").extract()[0]
            # print(src)
            yield scrapy.Request(url=src,callback=self.parse_second,meta={"name":name})     #发起一个新的请求
    def parse_second(self,response):
        name=response.meta.get("name")
        # print(name)
        src=response.xpath("//div[@id='Zoom']//img/@src").extract_first()
        # print(src)
        movie=ProjectNameItem(name=name,src=src)
        yield  movie
```

## items.py

```
name=scrapy.Field()
src=scrapy.Field()
```

## pipelines.py

```
import urllib.request
class ProjectNamePipeline:
    def open_spider(self,spider):       #只执行一次，程序开始时
        self.file=open("movies.json","w",encoding="utf-8")
    def process_item(self, item, spider):
        self.file.write(str(item))
        print(item)
        return item                     #返回给下一个管道类继续调用
    def close_spider(self,spider):      #只执行一次，当整个程序关闭时
        self.file.close()
class DownData:
    def process_item(self,item,spider):
        url=item.get("src")
        file_name=item.get("name")
        urllib.request.urlretrieve(url,f"./movies/{file_name}.jpg")
        print(url)
        print(file_name)
'''
管道A  300          
管道B  301
    
    如上，管道A的优先级高于管道B的优先级，
        -那么当A执行完一次process_item方法时，就会紧接着执行B中的process_item方法。
        -同理，当A执行完一次open_spider方法时，就会紧接着执行B中的open_spider方法。
        -但是close_spider方法，是优先级低的先执行，所以先执行B中的close_spider方法，
            然后紧接着执行A中的close_spider方法
        -注意了：
            所有的open_spider方法和close_spider方法，都只会执行一次。
            -open_spider()      #程序启动时执行
            -close_spider()     #程序关闭时执行
        -process_item()可以执行多次，yield执行几次，它就执行几次。
'''
```

## settings.py

```
LOG_LEVEL="WARNING"
ITEM_PIPELINES = {
   'project_name.pipelines.ProjectNamePipeline': 300,
    'project_name.pipelines.DownData': 301,
}
```