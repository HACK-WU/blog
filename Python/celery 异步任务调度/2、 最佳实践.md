**创建项目celerypro**

**创建异步任务执行文件celery_task:**[](javascript:void(0);)

```python
**import celery**
**import time**
**backend = 'redis://127.0.0.1:6379/1'**
**broker = 'redis://127.0.0.1:6379/2'**
**cel = celery.Celery('test', backend=backend, broker=broker)**
**@cel.task**
**def send_email(name):**
**    print("向%s发送邮件..." % name)**
**    time.sleep(5)**
**    print("向%s发送邮件完成" % name)**
**    return "ok"**
**@cel.task**
**def send_sms(name):**
**    print("向%s发送短信..." % name)**
**    time.sleep(5)**
**    print("向%s发送短信完成" % name)**
**    return "ok"　**
```

[](javascript:void(0);)

**创建执行任务文件,produce_task.py:**

```python
**from celery_task import send_email, send_sms**
**result = send_email.delay('hack')**
**print(result.id)**
**result = send_sms.delay('hack')**
**print(result)**
**　**
```

**注意，异步任务文件命令执行：**

```shell
**celery worker -A main -l info **
**# 如果是4.x 版本，且在windows上执行。 需要： pip install eventlet**
**# 然后： celery worker -A main -l info -P eventlet**
```

**创建py文件：result.py，查看任务执行结果，**[](javascript:void(0);)

```python
**from celery.result import AsyncResult**
**from celery_task import cel**
**async_result=AsyncResult(id="c6ddd5b7-a662-4f0e-93d4-ab69ec2aea5d", app=cel)**
**if async_result.successful():**
**    result = async_result.get()**
**    print(result)**
**    # result.forget() # 将结果删除**
**elif async_result.failed():**
**    print('执行失败')**
**elif async_result.status == 'PENDING':**
**    print('任务等待中被执行')**
**elif async_result.status == 'RETRY':**
**    print('任务异常后正在重试')**
**elif async_result.status == 'STARTED':**
**    print('任务已经开始被执行')**
```

# **celery 使用帮助信息**

```javascript
**Usage: celery worker [options]**
** **
**Start worker instance.**
** **
**Examples::**
** **
**    celery worker --app=proj -l info**
**    celery worker -A proj -l info -Q hipri,lopri**
** **
**    celery worker -A proj --concurrency=4**
**    celery worker -A proj --concurrency=1000 -P eventlet**
** **
**    celery worker --autoscale=10,0**
** **
**Options:**
**  -A APP, --app=APP     app instance to use (e.g. module.attr_name)**
**  -b BROKER, --broker=BROKER**
**                        url to broker.  default is 'amqp://guest@localhost//'**
**  --loader=LOADER       name of custom loader class to use.**
**  --config=CONFIG       Name of the configuration module**
**  --workdir=WORKING_DIRECTORY**
**                        Optional directory to change to after detaching.**
**  -C, --no-color**
**  -q, --quiet**
**  -c CONCURRENCY, --concurrency=CONCURRENCY**
**                        Number of child processes processing the queue. The**
**                        default is the number of CPUs available on your**
**                        system.**
**  -P POOL_CLS, --pool=POOL_CLS**
**                        Pool implementation: prefork (default), eventlet,**
**                        gevent, solo or threads.**
**  --purge, --discard    Purges all waiting tasks before the daemon is started.**
**                        **WARNING**: This is unrecoverable, and the tasks will**
**                        be deleted from the messaging server.**
**  -l LOGLEVEL, --loglevel=LOGLEVEL**
**                        Logging level, choose between DEBUG, INFO, WARNING,**
**                        ERROR, CRITICAL, or FATAL.**
**  -n HOSTNAME, --hostname=HOSTNAME**
**                        Set custom hostname, e.g. 'w1.%h'. Expands: %h**
**                        (hostname), %n (name) and %d, (domain).**
**  -B, --beat            Also run the celery beat periodic task scheduler.**
**                        Please note that there must only be one instance of**
**                        this service.**
**  -s SCHEDULE_FILENAME, --schedule=SCHEDULE_FILENAME**
**                        Path to the schedule database if running with the -B**
**                        option. Defaults to celerybeat-schedule. The extension**
**                        ".db" may be appended to the filename. Apply**
**                        optimization profile.  Supported: default, fair**
**  --scheduler=SCHEDULER_CLS**
**                        Scheduler class to use. Default is**
**                        celery.beat.PersistentScheduler**
**  -S STATE_DB, --statedb=STATE_DB**
**                        Path to the state database. The extension '.db' may be**
**                        appended to the filename. Default: None**
**  -E, --events          Send events that can be captured by monitors like**
**                        celery events, celerymon, and others.**
**  --time-limit=TASK_TIME_LIMIT**
**                        Enables a hard time limit (in seconds int/float) for**
**                        tasks.**
**  --soft-time-limit=TASK_SOFT_TIME_LIMIT**
**                        Enables a soft time limit (in seconds int/float) for**
**                        tasks.**
**  --maxtasksperchild=MAX_TASKS_PER_CHILD**
**                        Maximum number of tasks a pool worker can execute**
**                        before it's terminated and replaced by a new worker.**
**  -Q QUEUES, --queues=QUEUES**
**                        List of queues to enable for this worker, separated by**
**                        comma. By default all configured queues are enabled.**
**                        Example: -Q video,image**
**  -X EXCLUDE_QUEUES, --exclude-queues=EXCLUDE_QUEUES**
**  -I INCLUDE, --include=INCLUDE**
**                        Comma separated list of additional modules to import.**
**                        Example: -I foo.tasks,bar.tasks**
**  --autoscale=AUTOSCALE**
**                        Enable autoscaling by providing max_concurrency,**
**                        min_concurrency. Example:: --autoscale=10,3 (always**
**                        keep 3 processes, but grow to 10 if necessary)**
**  --autoreload          Enable autoreloading.**
**  --no-execv            Don't do execv after multiprocessing child fork.**
**  --without-gossip      Do not subscribe to other workers events.**
**  --without-mingle      Do not synchronize with other workers at startup.**
**  --without-heartbeat   Do not send event heartbeats.**
**  --heartbeat-interval=HEARTBEAT_INTERVAL**
**                        Interval in seconds at which to send worker heartbeat**
**  -O OPTIMIZATION**
**  -D, --detach**
**  -f LOGFILE, --logfile=LOGFILE**
**                        Path to log file. If no logfile is specified, stderr**
**                        is used.**
**  --pidfile=PIDFILE     Optional file used to store the process pid. The**
**                        program will not start if this file already exists and**
**                        the pid is still alive.**
**  --uid=UID             User id, or user name of the user to run as after**
**                        detaching.**
**  --gid=GID             Group id, or group name of the main group to change to**
**                        after detaching.**
**  --umask=UMASK         Effective umask (in octal) of the process after**
**                        detaching.  Inherits the umask of the parent process**
**                        by default.**
**  --executable=EXECUTABLE**
**                        Executable to use for the detached process.**
**  --version             show program's version number and exit**
**  -h, --help            show this help message and exit**
```